# Tool Output: tool_c783147ac001G1QRTxDDsYaQ7h
**Date**: 2026-02-19 23:17:01 UTC
**Size**: 683,618 bytes

```
[
  {
    "status": "fulfilled",
    "value": {
      "url": "https://arxiv.org/html/2505.17989v1",
      "content": " Outcome-based Reinforcement Learning to Predict the Future      \n\n1.  [1 Introduction](https://arxiv.org/html/2505.17989v1#S1 \"In Outcome-based Reinforcement Learning to Predict the Future\")\n2.  [2 Method](https://arxiv.org/html/2505.17989v1#S2 \"In Outcome-based Reinforcement Learning to Predict the Future\")\n    1.  [2.1 Data and Preprocessing](https://arxiv.org/html/2505.17989v1#S2.SS1 \"In 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n    2.  [2.2 Model](https://arxiv.org/html/2505.17989v1#S2.SS2 \"In 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n    3.  [2.3 Reward Design](https://arxiv.org/html/2505.17989v1#S2.SS3 \"In 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n        1.  [2.3.1 RL Algorithms and Updates](https://arxiv.org/html/2505.17989v1#S2.SS3.SSS1 \"In 2.3 Reward Design â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n            1.  [GRPO](https://arxiv.org/html/2505.17989v1#S2.SS3.SSS1.Px1 \"In 2.3.1 RL Algorithms and Updates â€£ 2.3 Reward Design â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n            2.  [Modified GRPO](https://arxiv.org/html/2505.17989v1#S2.SS3.SSS1.Px2 \"In 2.3.1 RL Algorithms and Updates â€£ 2.3 Reward Design â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n            3.  [ReMax](https://arxiv.org/html/2505.17989v1#S2.SS3.SSS1.Px3 \"In 2.3.1 RL Algorithms and Updates â€£ 2.3 Reward Design â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n            4.  [DPO](https://arxiv.org/html/2505.17989v1#S2.SS3.SSS1.Px4 \"In 2.3.1 RL Algorithms and Updates â€£ 2.3 Reward Design â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n            5.  [General Remarks and Details](https://arxiv.org/html/2505.17989v1#S2.SS3.SSS1.Px5 \"In 2.3.1 RL Algorithms and Updates â€£ 2.3 Reward Design â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n    4.  [2.4 Training Protocol and Stability Measures](https://arxiv.org/html/2505.17989v1#S2.SS4 \"In 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n        1.  [Online, single-pass training](https://arxiv.org/html/2505.17989v1#S2.SS4.SSS0.Px1 \"In 2.4 Training Protocol and Stability Measures â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n        2.  [Guard-rails and failure modes](https://arxiv.org/html/2505.17989v1#S2.SS4.SSS0.Px2 \"In 2.4 Training Protocol and Stability Measures â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n        3.  [Baselines](https://arxiv.org/html/2505.17989v1#S2.SS4.SSS0.Px3 \"In 2.4 Training Protocol and Stability Measures â€£ 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n    5.  [2.5 Failure Modes](https://arxiv.org/html/2505.17989v1#S2.SS5 \"In 2 Method â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n3.  [3 Results](https://arxiv.org/html/2505.17989v1#S3 \"In Outcome-based Reinforcement Learning to Predict the Future\")\n    1.  [3.1 Training Algorithm Comparisons](https://arxiv.org/html/2505.17989v1#S3.SS1 \"In 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n    2.  [3.2 Main Model Accuracy Comparison.](https://arxiv.org/html/2505.17989v1#S3.SS2 \"In 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n    3.  [3.3 Hypothetical Trading Evaluation](https://arxiv.org/html/2505.17989v1#S3.SS3 \"In 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n        1.  [Total profit differences](https://arxiv.org/html/2505.17989v1#S3.SS3.SSS0.Px1 \"In 3.3 Hypothetical Trading Evaluation â€£ 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n        2.  [Per-trade profit differences.](https://arxiv.org/html/2505.17989v1#S3.SS3.SSS0.Px2 \"In 3.3 Hypothetical Trading Evaluation â€£ 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n        3.  [Practical takeaway.](https://arxiv.org/html/2505.17989v1#S3.SS3.SSS0.Px3 \"In 3.3 Hypothetical Trading Evaluation â€£ 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\")\n4.  [4 Conclusion](https://arxiv.org/html/2505.17989v1#S4 \"In Outcome-based Reinforcement Learning to Predict the Future\")\n\n# Outcome-based Reinforcement Learning  \nto Predict the Future\n\nBenjamin Turtel Lightning Rod Labs Danny Franklin Lightning Rod Labs Kris Skotheim Lightning Rod LabsLuke Hewitt Stanford University Philipp Schoenegger London School of Economics and Political Science\n\n###### Abstract\n\nReinforcement learning with verifiable rewards (RLVR) has boosted math and coding in large language models, yet there has been little effort to extend RLVR into messier, real-world domains like forecasting. One sticking point is that outcome-based reinforcement learning for forecasting must learn from binary, delayed, and noisy rewards, a regime where standard fine-tuning is brittle. We show that outcome-only online RL on a 14B model can match frontier-scale accuracy and surpass it in calibration and hypothetical prediction market betting by adapting two leading algorithms, Group-Relative Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our adaptations remove per-question variance scaling in GRPO, apply baseline-subtracted advantages in ReMax, hydrate training with 100k temporally consistent synthetic questions, and introduce lightweight guard-rails that penalise gibberish, non-English responses and missing rationales, enabling a single stable pass over 110k events. Scaling ReMax to 110k questions and ensembling seven predictions yields a 14B model that matches frontier baseline o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this calibration edge into $127 of hypothetical profit versus $92 for o1 (p = 0.037). This demonstrates that refined RLVR methods can convert small-scale LLMs into potentially economically valuable forecasting tools, with implications for scaling this to larger models.\n\n## 1 Introduction\n\n111We thank Rich Collins for data verification and fact-checking.\n\nReinforcement learning with verifiable rewards (RLVR) has recently boosted large-language-model (LLM) performance on benchmarks such as GSM8K, AIME, and MATH(Zhao etal., [2025a](https://arxiv.org/html/2505.17989v1#bib.bib17); Shao etal., [2024](https://arxiv.org/html/2505.17989v1#bib.bib11); Guo etal., [2025](https://arxiv.org/html/2505.17989v1#bib.bib3); Lambert etal., [2024](https://arxiv.org/html/2505.17989v1#bib.bib4); Xie etal., [2025](https://arxiv.org/html/2505.17989v1#bib.bib15)). RLVR fine-tunes a base model with on-policy reinforcement learning (RL) against objective pass/fail signals, like unit tests or exact numeric solutions(Liu etal., [2023](https://arxiv.org/html/2505.17989v1#bib.bib6); Su etal., [2025](https://arxiv.org/html/2505.17989v1#bib.bib12)). Complementary self-learning work shows that mixing verifiable non-math corpora into RL pipelines can extend these gains beyond purely symbolic tasks(Akter etal., [2025](https://arxiv.org/html/2505.17989v1#bib.bib1)).\n\nHowever, it remains unclear how well such improvements transfer to messy, real-world reasoning problems whose outcomes are noisy, delayed, and probabilistic rather than deterministic and instantly verifiable(Murphy, [2025](https://arxiv.org/html/2505.17989v1#bib.bib8)). Forecasting exemplifies this challenge: it demands causal inference, trend extrapolation, and well-calibrated probabilities while supplying only sparse, lagged supervision(Weng, [2024](https://arxiv.org/html/2505.17989v1#bib.bib14); Qu etal., [2025](https://arxiv.org/html/2505.17989v1#bib.bib9)). Under these conditions standard GRPO-style updates can drive policies toward extreme overconfidence, gibberish output, or outright training collapse. Adapting RLVR to this setting promises to extend model reasoning ability to an especially demanding domain, potentially unlocking a host of real-world applications.\n\nWe design and empirically validate a stable RL pipeline for probabilistic forecasting. On the algorithmic side, we (i) remove per-question standard-deviation scaling from Group Relative Policy Optimisation (GRPO), (ii) switch to baseline-subtracted advantages in ReMax(Li etal., [2023](https://arxiv.org/html/2505.17989v1#bib.bib5)), and (iii) add lightweight guard-rails, token-length limits, a gibberish filter, and an early-stop criterion, to keep gradients proportional to Brier loss and prevent collapse over more than 100,000 sequential events. On the evaluation side, we measure accuracy with the soft-Brier score and calibration with expected calibration error (ECE) on a strictly time-ordered 3,300-question Polymarket hold-out, and we quantify economic value by converting each forecast into a set of hypothetical trades and comparing realised profits with those of the frontier reasoning model o1 as a benchmark.\n\nWe find that on the 3300-question hold-out set, a seven-run ReMax ensemble attains a Brier of 0.193\\[0.186,0.200\\]0.1930.1860.2000.193\\\\,\\[0.186,0.200\\]0.193 \\[ 0.186 , 0.200 \\] and an ECE of 0.0420.0420.0420.042, which is not statistically different from o1 in accuracy (+0.0030.003+0.003\\+ 0.003, p\\=.23ğ‘.23p=.23italic\\_p = .23) while _halving_ its mis-calibration (âˆ’0.0470.047\\-0.047\\- 0.047, p<.001ğ‘.001p<.001italic\\_p < .001). With a trading strategy that only places a hypothetical bet when the modelâ€™s predicted edge exceeds its own expected-calibration error, ReMax realises $127currency-dollar127\\\\$127$ 127 versus $92currency-dollar92\\\\$92$ 92 for o1, being restricted to a single $1 share per market. In total hypothetical profit, this surplus of $35currency-dollar35\\\\$35$ 35 is statistically significant (95 % CI +$2.0currency-dollar2.0+\\\\$2.0\\+ $ 2.0 to +$68.6currency-dollar68.6+\\\\$68.6\\+ $ 68.6, p\\=0.037ğ‘0.037p=0.037italic\\_p = 0.037). Thus our ReMax ensemble is on par with frontier model accuracy, reaches near-market-level calibration, and leverages that calibration edge in a hypothetical trading strategy to outperform o1.\n\n## 2 Method\n\n### 2.1 Data and Preprocessing\n\nWe construct a time\\-ordered corpus by merging two sources of questions. First, we collect roughly 10,000 resolved yes/no contracts from Polymarket, including creation date, close date, resolution timestamp and final outcome. Second, we generate 100,000 additional forecasting prompts with Lightning Rod Labsâ€™ proprietary Foresight Learning framework, generating high-quality training data without humans in the loop or labeled data. The final dataset covers multiple topical domains ranging from macroeconomics, weather, and culture to technology and politics. Each question is binary (resolves as yes or no) and has a probability attached to it (market price or model forecast). All training examples resolve before the earliest test question, fully eliminating look-ahead bias. For every contract we draw a single â€œprediction timestampâ€ uniformly at random between its on-chain open and scheduled close; all external context is truncated at 00:00 UTC on that calendar day. News headlines that we provide the models with at prediction time are retrieved through the Exa.ai API, which offers day-level granularity, so the model never sees information dated on or after its own forecast.\n\nWe first train a set of models with the initial 10,000 question set. In a second set of experiments, we use the same initial 10,000 questions with the addition 100,000 synthetic questions mixed in and time-ordered. We reserve a held-out test set of 3,300 questions for all models.\n\n### 2.2 Model\n\nAll experiments are conducted with DeepSeek-R1-Distill-Qwen-14B, a 14-billion-parameter model initialised from the open-weight Qwen 2.5-14B base checkpoint (Yang etal., [2024](https://arxiv.org/html/2505.17989v1#bib.bib16)) and further instruction-tuned on the 800k-sample DeepSeek-R1 distilled reasoning corpus(Zhao etal., [2025b](https://arxiv.org/html/2505.17989v1#bib.bib18)). Our fine-tuning regimes take this as the base model and our analyses compare to this base model as well as other benchmarks and frontier models.\n\n### 2.3 Reward Design\n\nWe treat the forecasting task as a sequential decision problem in which the model receives a textual prompt about a future event, outputs a single probability p^âˆˆ\\[0,1\\]^ğ‘01\\\\hat{p}\\\\in\\[0,1\\]over^ start\\_ARG italic\\_p end\\_ARG âˆˆ \\[ 0 , 1 \\] that the event will occur, and subsequently observes the binary outcome yâˆˆ{0,1}ğ‘¦01y\\\\in\\\\{0,1\\\\}italic\\_y âˆˆ { 0 , 1 }. The episode reward is the (negative) Brier score(Brier, [1950](https://arxiv.org/html/2505.17989v1#bib.bib2))\n\nR\\=âˆ’(p^âˆ’y)2,ğ‘…superscript^ğ‘ğ‘¦2R\\\\;=\\\\;-\\\\,(\\\\hat{p}-y)^{2},italic\\_R = - ( over^ start\\_ARG italic\\_p end\\_ARG - italic\\_y ) start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT ,\n\na strictly proper scoring rule that incentivises calibrated probabilistic forecasts.\n\nModel outputs occasionally fail to parse as a valid probability, for example when the generation omits a numeric answer or when the model fails to answer altogether. During training, we use the following scheme:\n\n*   â€¢\n    \n    Strict Brier: assign the maximum Brier loss of 1.01.01.01.0 (equivalently a training reward R\\=âˆ’1ğ‘…1R=-1italic\\_R = - 1) whenever the regex fails to extract a probability in \\[0,1\\]01\\[0,1\\]\\[ 0 , 1 \\].\n    \n\nFor evaluation, we use the following metric that does not penalise misformatting and missing predictions as much:\n\n*   â€¢\n    \n    Soft Brier: assign a constant Brier loss of 0.250.250.250.25 (training reward R\\=âˆ’0.25ğ‘…0.25R=-0.25italic\\_R = - 0.25) to any malformed or missing forecast, preserving gradient signal while avoiding training collapse.\n    \n\n#### 2.3.1 RL Algorithms and Updates\n\nWe evaluate three on-policy RL algorithms, GRPO, Modified-GRPO and ReMax, and include Direct Preference Optimisation (DPO) as an off-policy baseline.\n\n##### GRPO\n\nFirst, we test Group Relative Policy Optimization (GRPO)(Shao etal., [2024](https://arxiv.org/html/2505.17989v1#bib.bib11)). For each question qğ‘qitalic\\_q we draw GğºGitalic\\_G outputs {o1,â€¦,oG}superscriptğ‘œ1â€¦superscriptğ‘œğº\\\\{o^{1},\\\\dots,o^{G}\\\\}{ italic\\_o start\\_POSTSUPERSCRIPT 1 end\\_POSTSUPERSCRIPT , â€¦ , italic\\_o start\\_POSTSUPERSCRIPT italic\\_G end\\_POSTSUPERSCRIPT } from the old policy Ï€Î¸old(â‹…âˆ£q)\\\\pi\\_{\\\\theta\\_{\\\\mathrm{old}}}(\\\\cdot\\\\mid q)italic\\_Ï€ start\\_POSTSUBSCRIPT italic\\_Î¸ start\\_POSTSUBSCRIPT roman\\_old end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT ( â‹… âˆ£ italic\\_q ) and assign each output oisuperscriptğ‘œğ‘–o^{i}italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT a scalar reward risuperscriptğ‘Ÿğ‘–r^{i}italic\\_r start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT. Let\n\nÎ¼\\=1Gâˆ‘i\\=1Gri,Ïƒ\\=1Gâˆ‘i\\=1G(riâˆ’Î¼)2,A^i\\=riâˆ’Î¼Ïƒ.formulae-sequenceğœ‡1ğºsuperscriptsubscriptğ‘–1ğºsuperscriptğ‘Ÿğ‘–formulae-sequenceğœ1ğºsuperscriptsubscriptğ‘–1ğºsuperscriptsuperscriptğ‘Ÿğ‘–ğœ‡2superscript^ğ´ğ‘–superscriptğ‘Ÿğ‘–ğœ‡ğœ\\\\mu=\\\\frac{1}{G}\\\\sum\\_{i=1}^{G}r^{i},\\\\qquad\\\\sigma=\\\\sqrt{\\\\frac{1}{G}\\\\sum\\_{i=1}^{G% }(r^{i}-\\\\mu)^{2}},\\\\qquad\\\\hat{A}^{i}=\\\\frac{r^{i}-\\\\mu}{\\\\sigma}.italic\\_Î¼ = divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_G end\\_ARG âˆ‘ start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_G end\\_POSTSUPERSCRIPT italic\\_r start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT , italic\\_Ïƒ = square-root start\\_ARG divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_G end\\_ARG âˆ‘ start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_G end\\_POSTSUPERSCRIPT ( italic\\_r start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT - italic\\_Î¼ ) start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT end\\_ARG , over^ start\\_ARG italic\\_A end\\_ARG start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT = divide start\\_ARG italic\\_r start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT - italic\\_Î¼ end\\_ARG start\\_ARG italic\\_Ïƒ end\\_ARG .\n\nThe policy is updated by maximising\n\nğ’¥GRPO(Î¸)\\=ğ”¼q,{oi}\\[1Gâˆ‘i\\=1G1|oi|âˆ‘t\\=1|oi|min(ri,tA^i,clip(ri,t,1âˆ’Îµ,1+Îµ)A^i)âˆ’Î²DKL(Ï€Î¸âˆ¥Ï€ref)\\],subscriptğ’¥GRPOğœƒsubscriptğ”¼ğ‘superscriptğ‘œğ‘–delimited-\\[\\]1ğºsuperscriptsubscriptğ‘–1ğº1superscriptğ‘œğ‘–superscriptsubscriptğ‘¡1superscriptğ‘œğ‘–subscriptğ‘Ÿğ‘–ğ‘¡superscript^ğ´ğ‘–clipsubscriptğ‘Ÿğ‘–ğ‘¡1ğœ€1ğœ€superscript^ğ´ğ‘–ğ›½subscriptğ·KLconditionalsubscriptğœ‹ğœƒsubscriptğœ‹ref\\\\mathcal{J}\\_{\\\\mathrm{GRPO}}(\\\\theta)=\\\\mathbb{E}\\_{q,\\\\{o^{i}\\\\}}\\\\!\\\\Bigl{\\[}\\\\frac{1}% {G}\\\\sum\\_{i=1}^{G}\\\\frac{1}{|o^{i}|}\\\\sum\\_{t=1}^{|o^{i}|}\\\\min\\\\!\\\\bigl{(}r\\_{i,t}% \\\\hat{A}^{i},\\\\operatorname{clip}(r\\_{i,t},1-\\\\varepsilon,1+\\\\varepsilon)\\\\hat{A}^{i% }\\\\bigr{)}-\\\\beta\\\\,D\\_{\\\\mathrm{KL}}(\\\\pi\\_{\\\\theta}\\\\!\\\\|\\\\!\\\\pi\\_{\\\\mathrm{ref}})\\\\Bigr{\\]},caligraphic\\_J start\\_POSTSUBSCRIPT roman\\_GRPO end\\_POSTSUBSCRIPT ( italic\\_Î¸ ) = blackboard\\_E start\\_POSTSUBSCRIPT italic\\_q , { italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT } end\\_POSTSUBSCRIPT \\[ divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_G end\\_ARG âˆ‘ start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_G end\\_POSTSUPERSCRIPT divide start\\_ARG 1 end\\_ARG start\\_ARG | italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT | end\\_ARG âˆ‘ start\\_POSTSUBSCRIPT italic\\_t = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT | italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT | end\\_POSTSUPERSCRIPT roman\\_min ( italic\\_r start\\_POSTSUBSCRIPT italic\\_i , italic\\_t end\\_POSTSUBSCRIPT over^ start\\_ARG italic\\_A end\\_ARG start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT , roman\\_clip ( italic\\_r start\\_POSTSUBSCRIPT italic\\_i , italic\\_t end\\_POSTSUBSCRIPT , 1 - italic\\_Îµ , 1 + italic\\_Îµ ) over^ start\\_ARG italic\\_A end\\_ARG start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT ) - italic\\_Î² italic\\_D start\\_POSTSUBSCRIPT roman\\_KL end\\_POSTSUBSCRIPT ( italic\\_Ï€ start\\_POSTSUBSCRIPT italic\\_Î¸ end\\_POSTSUBSCRIPT âˆ¥ italic\\_Ï€ start\\_POSTSUBSCRIPT roman\\_ref end\\_POSTSUBSCRIPT ) \\] ,\n\nwhere\n\nri,t\\=Ï€Î¸(otiâˆ£q,o<ti)Ï€Î¸old(otiâˆ£q,o<ti),subscriptğ‘Ÿğ‘–ğ‘¡subscriptğœ‹ğœƒconditionalsubscriptsuperscriptğ‘œğ‘–ğ‘¡ğ‘subscriptsuperscriptğ‘œğ‘–absentğ‘¡subscriptğœ‹subscriptğœƒoldconditionalsubscriptsuperscriptğ‘œğ‘–ğ‘¡ğ‘subscriptsuperscriptğ‘œğ‘–absentğ‘¡r\\_{i,t}=\\\\frac{\\\\pi\\_{\\\\theta}(o^{i}\\_{t}\\\\mid q,o^{i}\\_{<t})}{\\\\pi\\_{\\\\theta\\_{\\\\mathrm{% old}}}(o^{i}\\_{t}\\\\mid q,o^{i}\\_{<t})},italic\\_r start\\_POSTSUBSCRIPT italic\\_i , italic\\_t end\\_POSTSUBSCRIPT = divide start\\_ARG italic\\_Ï€ start\\_POSTSUBSCRIPT italic\\_Î¸ end\\_POSTSUBSCRIPT ( italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT âˆ£ italic\\_q , italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT < italic\\_t end\\_POSTSUBSCRIPT ) end\\_ARG start\\_ARG italic\\_Ï€ start\\_POSTSUBSCRIPT italic\\_Î¸ start\\_POSTSUBSCRIPT roman\\_old end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT ( italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT âˆ£ italic\\_q , italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT < italic\\_t end\\_POSTSUBSCRIPT ) end\\_ARG ,\n\nÎµğœ€\\\\varepsilonitalic\\_Îµ is the clipping threshold, Ï€refsubscriptğœ‹ref\\\\pi\\_{\\\\mathrm{ref}}italic\\_Ï€ start\\_POSTSUBSCRIPT roman\\_ref end\\_POSTSUBSCRIPT is the reference policy reset to the current policy at the start of each outer iteration, and Î²ğ›½\\\\betaitalic\\_Î² is the KL-penalty coefficient. Normalising by Ïƒğœ\\\\sigmaitalic\\_Ïƒ stabilises the updates, but can dampen the learning signal when particularly large rewards are important.\n\n##### Modified GRPO\n\nSecond, we modify the standard GRPO algorithm by removing the standard-deviation division and set A^i\\=riâˆ’Î¼superscript^ğ´ğ‘–superscriptğ‘Ÿğ‘–ğœ‡\\\\hat{A}^{i}=r^{i}-\\\\muover^ start\\_ARG italic\\_A end\\_ARG start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT = italic\\_r start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT - italic\\_Î¼. This preserves the raw magnitude of especially large forecast errors, potentially improving the modelâ€™s ability to correct extreme miscalibrations. Yet, omitting the normalisation can make optimisation more sensitive to outliers, requiring additional guard-rails to mitigate instability.222This vulnerability is partly mitigated in our setting because Brier scores are intrinsically bounded in \\[0,1\\]01\\[0,1\\]\\[ 0 , 1 \\], which caps the variance of individual rewards and makes the absence of per-question normalisation far less destabilising than it would be for tasks with unbounded numeric losses. This modification parallels the Dr.GRPO method proposed by Liu et al.(Liu etal., [2025](https://arxiv.org/html/2505.17989v1#bib.bib7)).\n\n##### ReMax\n\nThird, we also apply ReMax(Li etal., [2023](https://arxiv.org/html/2505.17989v1#bib.bib5)) by first sampling GğºGitalic\\_G outputs {o1,â€¦,oG}superscriptğ‘œ1â€¦superscriptğ‘œğº\\\\{o^{1},\\\\dots,o^{G}\\\\}{ italic\\_o start\\_POSTSUPERSCRIPT 1 end\\_POSTSUPERSCRIPT , â€¦ , italic\\_o start\\_POSTSUPERSCRIPT italic\\_G end\\_POSTSUPERSCRIPT } from the old policy Ï€Î¸old(â‹…âˆ£q)\\\\pi\\_{\\\\theta\\_{\\\\mathrm{old}}}(\\\\cdot\\\\mid q)italic\\_Ï€ start\\_POSTSUBSCRIPT italic\\_Î¸ start\\_POSTSUBSCRIPT roman\\_old end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT ( â‹… âˆ£ italic\\_q ), each with reward risuperscriptğ‘Ÿğ‘–r^{i}italic\\_r start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT. Let bisuperscriptğ‘ğ‘–b^{i}italic\\_b start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT be a learned baseline for each oisuperscriptğ‘œğ‘–o^{i}italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT. We set the advantage to A^i\\=riâˆ’bisuperscript^ğ´ğ‘–superscriptğ‘Ÿğ‘–superscriptğ‘ğ‘–\\\\hat{A}^{i}=r^{i}-b^{i}over^ start\\_ARG italic\\_A end\\_ARG start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT = italic\\_r start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT - italic\\_b start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT, removing the need to divide by a standard deviation. The policy is then updated by maximising\n\nğ’¥ReMax(Î¸)\\=ğ”¼q,{oi}\\[1Gâˆ‘i\\=1GA^iâˆ‘t\\=1|oi|logÏ€Î¸(otiâˆ£q,o<ti)âˆ’Î²DKL(Ï€Î¸âˆ¥Ï€ref)\\],subscriptğ’¥ReMaxğœƒsubscriptğ”¼ğ‘superscriptğ‘œğ‘–delimited-\\[\\]1ğºsuperscriptsubscriptğ‘–1ğºsuperscript^ğ´ğ‘–superscriptsubscriptğ‘¡1superscriptğ‘œğ‘–subscriptğœ‹ğœƒconditionalsubscriptsuperscriptğ‘œğ‘–ğ‘¡ğ‘subscriptsuperscriptğ‘œğ‘–absentğ‘¡ğ›½subscriptğ·KLconditionalsubscriptğœ‹ğœƒsubscriptğœ‹ref\\\\mathcal{J}\\_{\\\\mathrm{ReMax}}(\\\\theta)=\\\\mathbb{E}\\_{q,\\\\{o^{i}\\\\}}\\\\!\\\\Bigl{\\[}\\\\tfrac{% 1}{G}\\\\sum\\_{i=1}^{G}\\\\hat{A}^{i}\\\\sum\\_{t=1}^{|o^{i}|}\\\\log\\\\pi\\_{\\\\theta}(o^{i}\\_{t}% \\\\mid q,o^{i}\\_{<t})-\\\\beta\\\\,D\\_{\\\\mathrm{KL}}(\\\\pi\\_{\\\\theta}\\\\!\\\\|\\\\!\\\\pi\\_{\\\\mathrm{ref}}% )\\\\Bigr{\\]},caligraphic\\_J start\\_POSTSUBSCRIPT roman\\_ReMax end\\_POSTSUBSCRIPT ( italic\\_Î¸ ) = blackboard\\_E start\\_POSTSUBSCRIPT italic\\_q , { italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT } end\\_POSTSUBSCRIPT \\[ divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_G end\\_ARG âˆ‘ start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_G end\\_POSTSUPERSCRIPT over^ start\\_ARG italic\\_A end\\_ARG start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT âˆ‘ start\\_POSTSUBSCRIPT italic\\_t = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT | italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT | end\\_POSTSUPERSCRIPT roman\\_log italic\\_Ï€ start\\_POSTSUBSCRIPT italic\\_Î¸ end\\_POSTSUBSCRIPT ( italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT âˆ£ italic\\_q , italic\\_o start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT < italic\\_t end\\_POSTSUBSCRIPT ) - italic\\_Î² italic\\_D start\\_POSTSUBSCRIPT roman\\_KL end\\_POSTSUBSCRIPT ( italic\\_Ï€ start\\_POSTSUBSCRIPT italic\\_Î¸ end\\_POSTSUBSCRIPT âˆ¥ italic\\_Ï€ start\\_POSTSUBSCRIPT roman\\_ref end\\_POSTSUBSCRIPT ) \\] ,\n\nwhere Î²ğ›½\\\\betaitalic\\_Î² balances a KL penalty term. Subtracting a baseline in lieu of variance normalisation often better preserves large reward signals.\n\n##### DPO\n\nLastly, we also test Direct Preference Optimization(Rafailov etal., [2023](https://arxiv.org/html/2505.17989v1#bib.bib10)), a direct preference-based approach. This method has shown strong performance on QA tasks as well as forecasting tasks (Turtel etal., [2025](https://arxiv.org/html/2505.17989v1#bib.bib13)) and functions as a baseline for some of our analyses.\n\n##### General Remarks and Details\n\nIn our forecasting context, normalising each questionâ€™s rewards by their standard deviation (as in standard GRPO) can excessively dampen large errors and encourage overconfidence: per-question normalisation flattens the reward distribution and erases the natural asymmetry whereby modest gains accrue from correct but overconfident predictions, while rare mis-predictions incur disproportionately large penalties, the very signal the model needs to learn proper calibration. By removing per-question normalisation (Modified GRPO) or using ReMaxâ€™s baseline-based approach, we better preserves the impact of large deviations, improving calibration when probabilistic forecasts deviate significantly from eventual outcomes.\n\nAcross algorithms we keep the optimisation scaffold identical: AdamW (Î²1\\=0.9subscriptğ›½10.9\\\\beta\\_{1}=0.9italic\\_Î² start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT = 0.9, Î²2\\=0.999subscriptğ›½20.999\\\\beta\\_{2}=0.999italic\\_Î² start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT = 0.999, Ïµ\\=1Ã—10âˆ’8italic-Ïµ1superscript108\\\\epsilon=1\\\\times 10^{-8}italic\\_Ïµ = 1 Ã— 10 start\\_POSTSUPERSCRIPT - 8 end\\_POSTSUPERSCRIPT, no weight decay), bfloat16 precision, global grad-norm clip 1.01.01.01.0, and an entropy bonus coefficient of 0.0010.0010.0010.001. We modify only the levers each method cares about. GRPO uses an actor learning rate of 1Ã—10âˆ’61superscript1061\\\\times 10^{-6}1 Ã— 10 start\\_POSTSUPERSCRIPT - 6 end\\_POSTSUPERSCRIPT, an initial KL penalty of 0.0050.0050.0050.005, PPO ratio-clip Ïµ\\=0.20italic-Ïµ0.20\\\\epsilon=0.20italic\\_Ïµ = 0.20, and G\\=4ğº4G=4italic\\_G = 4 roll-outs per prompt; Modified-GRPO is identical except that it drops the Ïƒğœ\\\\sigmaitalic\\_Ïƒ division in the advantage to isolate the effect of normalisation. ReMax doubles the actor learning rate to 2Ã—10âˆ’62superscript1062\\\\times 10^{-6}2 Ã— 10 start\\_POSTSUPERSCRIPT - 6 end\\_POSTSUPERSCRIPT, keeps the KL schedule unchanged, and trains its learned value baseline with 1Ã—10âˆ’61superscript1061\\\\times 10^{-6}1 Ã— 10 start\\_POSTSUPERSCRIPT - 6 end\\_POSTSUPERSCRIPT under an MSE loss scaled by 0.50.50.50.5. DPO is run for 4444 epochs at Î²\\=0.1ğ›½0.1\\\\beta=0.1italic\\_Î² = 0.1 with a constant 1Ã—10âˆ’51superscript1051\\\\times 10^{-5}1 Ã— 10 start\\_POSTSUPERSCRIPT - 5 end\\_POSTSUPERSCRIPT learning rate and a batch size of 128128128128 sequences. All runs employ automatic mixed precision and gradient accumulation to emulate two sequences per GPU.\n\nAll experiments ran on a single 8-GPU node. The GRPO (10k), ReMax, Modified-GRPO, DPO, and the large-scale GRPO-100k run used eight NVIDIA H100 GPUs.\n\n### 2.4 Training Protocol and Stability Measures\n\n##### Online, single-pass training\n\nAll on-policy algorithms (GRPO, Modified-GRPO, ReMax) are trained strictly online: each question is encountered only once in chronological order, and its outcome is revealed immediately after the event date. We do not perform multiple epochs, as re-exposing the model to past questions after outcomes are known leads to severe over-fitting (the model essentially â€œlearns the futureâ€ on subsequent epochs).\n\n##### Guard-rails and failure modes\n\nScaling the training to 100k questions introduces stability challenges, particularly related to _overconfidence_, where some models drift toward 0% or 100% forecasts when per-question reward normalisation is used, and the generation of _invalid text or gibberish_, where large, noisy datasets can push the policy to produce nonsensical or non-English text. This happens especially when reward gradients fail to distinguish valid forecasts from invalid ones. To maintain stable training at scale, we use a scorer that deducts reward whenever a response violates any of three checks: it flags out-of-context non-English passages, nonsense or random character strings, and the absence of an explanation for the final answer inside a `<think>...</think>` block. During evaluation the scorer sets the Boolean fields contains\\_non\\_english, contains\\_gibberish, and explains\\_answer and records the continuous estimates non\\_english\\_proportionnon\\_english\\_proportion\\\\mathrm{non\\\\allowbreak\\\\\\_english\\\\allowbreak\\\\\\_proportion}roman\\_non \\_ roman\\_english \\_ roman\\_proportion, gibberish\\_proportiongibberish\\_proportion\\\\mathrm{gibberish\\\\allowbreak\\\\\\_proportion}roman\\_gibberish \\_ roman\\_proportion, and explanation\\_qualityexplanation\\_quality\\\\mathrm{explanation\\\\allowbreak\\\\\\_quality}roman\\_explanation \\_ roman\\_quality (each in \\[0,1\\]01\\[0,1\\]\\[ 0 , 1 \\]). The downstream reward subtracts penalties such as Î»langÃ—non\\_english\\_proportionsubscriptğœ†langnon\\_english\\_proportion\\\\lambda\\_{\\\\mathrm{lang}}\\\\times\\\\mathrm{non\\\\\\_english\\\\\\_proportion}italic\\_Î» start\\_POSTSUBSCRIPT roman\\_lang end\\_POSTSUBSCRIPT Ã— roman\\_non \\_ roman\\_english \\_ roman\\_proportion and Î»gibÃ—gibberish\\_proportionsubscriptğœ†gibgibberish\\_proportion\\\\lambda\\_{\\\\mathrm{gib}}\\\\times\\\\mathrm{gibberish\\\\\\_proportion}italic\\_Î» start\\_POSTSUBSCRIPT roman\\_gib end\\_POSTSUBSCRIPT Ã— roman\\_gibberish \\_ roman\\_proportion, applies a fixed penalty Î»misssubscriptğœ†miss\\\\lambda\\_{\\\\mathrm{miss}}italic\\_Î» start\\_POSTSUBSCRIPT roman\\_miss end\\_POSTSUBSCRIPT when explains\\_answer is false, and can add up to Î»expÃ—explanation\\_qualitysubscriptğœ†expexplanation\\_quality\\\\lambda\\_{\\\\mathrm{exp}}\\\\times\\\\mathrm{explanation\\\\\\_quality}italic\\_Î» start\\_POSTSUBSCRIPT roman\\_exp end\\_POSTSUBSCRIPT Ã— roman\\_explanation \\_ roman\\_quality when a rationale is present. The routine also hard-truncates any input beyond 16000 characters to keep evaluation stable, and its Pydantic schema validation guarantees that any formatting error or missing key zeroes the reward.\n\n##### Baselines\n\nWhile our primary focus is on comparing the four RL algorithms, we also benchmark forecasts against two additional references:\n\n1.  1.\n    \n    OpenAIâ€™s o1, prompted with the same question text, to assess performance against a frontier reasoning model.\n    \n2.  2.\n    \n    Market prices (Polymarket), the marketâ€™s implicit probability at the time each question was asked. For every test question, the Polymarket price is sampled at precisely the same timestamp used for the prompt cut-off, yielding a strictly contemporaneous benchmark.\n    \n\nThese baselines indicate how our RL-trained models perform relative to both a state-of-the-art commercial LLM and a real-world prediction market.\n\n### 2.5 Failure Modes\n\nWe observed a set of different failure modes. For example, in the model trained with GRPO without any guardrails, we find that the model sometimes assigns a probability of 0 to outcomes that are not impossible but rather unlikely. Asigning a probablity of 0 is not something a capable forecaster would do and should be avoided by forecasting models. For example, in response to the question â€œWill Trump say â€œUkraineâ€ 20+ times during Macron presser today?â€, the model outputs\n\n> \\[â€¦\\] In conclusion, while Ukraine will definitely be a topic, the number of mentions reaching 20 is astronomically low. Even if every statement and answer included \"Ukraine,\" itâ€™s improbable to reach that number. Therefore, my estimate is that the probability is extremely low, almost zero. `</think>` _\\*0.000\\*_.\n\nSimilarly, the model shows this overconfidence also on the other end of the probability spectrum. While acknowledging that it may have misunderstood something, it assigns probability of 1 to questions that the model itself is somewhat unsure about: â€œTherefore, unless Iâ€™m missing something, the probability is 1.â€ Overall, this model has 39.3% of predictions in either the 0â€“10% or the 90â€“100% bucket, with many landing exactly on 0 and 1, which shows a prevalence for very high or low probabilities.\n\nUsing our Modified GRPO approach, again without any guardrails, we do find that the model is more likely to avoid the extreme overconfidence of the standard GRPO approach, with only a total of 7.9% of predictions landing in either of the extreme buckets. However, we do observe a different failure mode. Some of the answers show language switching between Chinese and English within an output, â€œAlright, so I need toé¢„æµ‹ Patrick Mahomes åœ¨è¶…çº§ç¢— LXIX ä¸­æ˜¯å¦ä¼š rush 30+ç ï¼Œç»“æœä»¥è¶…è¿‡30ç ä¸ºâ€œOverâ€ï¼Œå¦åˆ™ä¸ºâ€œUnderâ€ã€‚é¦–å…ˆï¼Œæˆ‘ä¼šå›é¡¾æä¾›çš„æ–°é—»å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œæ‰¾å‡ºç›¸å…³æ•°æ®å’Œè¶‹åŠ¿â€, while others show partial segments of ungrammatical phrases and gibberish, such as â€œNext, I look at the â€¦think that would happen in this interview,â€ â€œI sâ€¦,â€ â€œLamar Jacâ€¦that,â€ and â€œThey talk about the theme, ticket distribution, and guests, but tâ€¦that theyâ€™re involved.â€\n\nWhen training the model with our set of guardrails in place on the 100k data set, both types of failure modes are less pronounced. For example, 13.1% of predictions fell into the 0â€“10% and 90â€“100% buckets, with many forecasts that were close to 0% now having values that are greater than 0. Similarly, gibberish responses are less likely, though some ungrammatical phrases and elisions remain, such as in â€œLooking aâ€¦ould impact the price.â€\n\n## 3 Results\n\n### 3.1 Training Algorithm Comparisons\n\nWe measure predictive accuracy with the _soft-Brier score_, defined as the squared error (p^âˆ’y)2superscript^ğ‘ğ‘¦2(\\\\hat{p}-y)^{2}( over^ start\\_ARG italic\\_p end\\_ARG - italic\\_y ) start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT averaged over all 3,300 questions but assigning a soft penalty of 0.250.250.250.25 whenever a model fails to produce a parseable probability. A score of 0.25 is equivalent to guessing 50% on a question, and thus functions as a suitable stand-in for failed responses. For calibration, we use the _expected calibration error_ (ECE) computed in ten equal-mass probability bins. For both accuracy and calibration, lower scores indicate higher accuracy and better calibration respectively. All statistics are paired across the identical question set (df\\=3294ğ‘‘ğ‘“3294df=3\\\\,294italic\\_d italic\\_f = 3 294); confidence intervals (CI) are two-sided 95%percent9595\\\\%95 % Wald intervals for Brier and bootstrap intervals for ECE unless specified otherwise; every pğ‘pitalic\\_p\\-value reported below is two-sided.\n\nAmong the 10k-trained models, ReMax (no guard-rails) is the most accurate with a soft-Brier score of 0.197 \\[0.190, 0.204\\]. DPO follows at 0.205 \\[0.197, 0.213\\] (Î”\\=0.008,p\\=0.002formulae-sequenceÎ”0.008ğ‘0.002\\\\Delta=0.008,\\\\;p=0.002roman\\_Î” = 0.008 , italic\\_p = 0.002), then Modified GRPO at 0.206 \\[0.199, 0.213\\] (Î”\\=0.009,p<0.001formulae-sequenceÎ”0.009ğ‘0.001\\\\Delta=0.009,\\\\;p<0.001roman\\_Î” = 0.009 , italic\\_p < 0.001), vanilla GRPO at 0.210 \\[0.200, 0.220\\] (Î”\\=0.013,p<0.001formulae-sequenceÎ”0.013ğ‘0.001\\\\Delta=0.013,\\\\;p<0.001roman\\_Î” = 0.013 , italic\\_p < 0.001) and the base model at 0.214 \\[0.207, 0.221\\] (Î”\\=0.017,p<0.001formulae-sequenceÎ”0.017ğ‘0.001\\\\Delta=0.017,\\\\;p<0.001roman\\_Î” = 0.017 , italic\\_p < 0.001). Calibration shows the same ordering: ReMax posts an ECE of 0.0507 \\[0.0381, 0.0633\\]; the nearest competitor, Modified GRPO, is higher by 0.046 ECE points \\[0.0325, 0.0599\\], p<0.001ğ‘0.001p<0.001italic\\_p < 0.001. DPO, vanilla GRPO, and the base model are still farther off, each differing from ReMax by at least 0.035 ECE points with p<0.001ğ‘0.001p<0.001italic\\_p < 0.001.\n\nScaling ReMax to the 100k Lightning corpus does not materially change performance unless an ensemble is used. The single 100k checkpoint records a Brier of 0.195 \\[0.188, 0.202\\] and an ECE of 0.0438 \\[0.0300, 0.0576\\]; both differences relative to the 10k baseline (Î”Brier\\=âˆ’0.002Î”Brier0.002\\\\Delta\\\\mathrm{Brier}=-0.002roman\\_Î” roman\\_Brier = - 0.002 \\[âˆ’0.006,0.0020.0060.002\\-0.006,0.002\\- 0.006 , 0.002\\], p\\=0.28ğ‘0.28p=0.28italic\\_p = 0.28; Î”ECE\\=âˆ’0.007Î”ECE0.007\\\\Delta\\\\mathrm{ECE}=-0.007roman\\_Î” roman\\_ECE = - 0.007 \\[âˆ’0.019,0.0060.0190.006\\-0.019,0.006\\- 0.019 , 0.006\\], p\\=0.28ğ‘0.28p=0.28italic\\_p = 0.28) are not statistically significant. Averaging seven such predictions trims the Brier further to 0.193 \\[0.186, 0.200\\], which is significantly lower than the 10k model (Î”\\=âˆ’0.004Î”0.004\\\\Delta=-0.004roman\\_Î” = - 0.004 \\[âˆ’0.008,âˆ’0.0010.0080.001\\-0.008,-0.001\\- 0.008 , - 0.001\\], p\\=0.018ğ‘0.018p=0.018italic\\_p = 0.018), while its ECE of 0.0424 \\[0.0301, 0.0547\\] is not significantly different (Î”\\=âˆ’0.008Î”0.008\\\\Delta=-0.008roman\\_Î” = - 0.008 \\[âˆ’0.021,0.0040.0210.004\\-0.021,0.004\\- 0.021 , 0.004\\], p\\=0.20ğ‘0.20p=0.20italic\\_p = 0.20).\n\nFigure[1](https://arxiv.org/html/2505.17989v1#S3.F1 \"Figure 1 â€£ 3.1 Training Algorithm Comparisons â€£ 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\") illustrates that ReMax, even at the 10k scale, closes roughly one-third of the accuracy gap between the instruction-tuned base model (Brier 0.214) and the real-money Polymarket benchmark (0.162) and delivers the most reliable probability forecasts among all training algorithms evaluated.\n\n![Refer to caption](extracted/6467676/algorithm_comparison.png)\n\nFigure 1: Mean soft-Brier score (accuracy, left) and mean expected calibration error (ECE, right) for each training algorithm on the Polymarket hold-out set. Error bars show 95%percent9595\\\\%95 % confidence intervals. Lower values are better on both axes.\n\n### 3.2 Main Model Accuracy Comparison.\n\nTable[1](https://arxiv.org/html/2505.17989v1#S3.T1 \"Table 1 â€£ 3.2 Main Model Accuracy Comparison. â€£ 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\") reports mean soft-Brier score and expected-calibration error (ECE) for the 7-run ReMax ensemble and Modified-GRPO (both trained on the 100k corpus) alongside the instruction-tuned base model (DeepSeek-R1 14B), the frontier baseline OpenAI o1, and contemporaneous Polymarket prices.\n\nTable 1: Accuracy summary and pair-wise tests (3295 questions).\n\nModel\n\nDescriptive means\n\nBrier difference (Î”Î”\\\\Deltaroman\\_Î” vs.)\n\nSoft-Brier\n\nECE\n\nBase\n\no1\n\nMarket\n\nPolymarket\n\n0.162\n\n0.0425\n\nâ€“\n\nâ€“\n\nâ€“\n\nReMax, Ensemble-7\n\n0.193\n\n0.0424\n\nâˆ’0.021âˆ—âˆ—âˆ—superscript0.021absent\\-0.021^{\\*\\*\\*}\\- 0.021 start\\_POSTSUPERSCRIPT âˆ— âˆ— âˆ— end\\_POSTSUPERSCRIPT\n\n+0.0030.003+0.003\\+ 0.003\n\n+0.031âˆ—âˆ—âˆ—superscript0.031absent+0.031^{\\*\\*\\*}\\+ 0.031 start\\_POSTSUPERSCRIPT âˆ— âˆ— âˆ— end\\_POSTSUPERSCRIPT\n\nOpenAI o1\n\n0.197\n\n0.0895\n\nâˆ’0.018âˆ—âˆ—âˆ—superscript0.018absent\\-0.018^{\\*\\*\\*}\\- 0.018 start\\_POSTSUPERSCRIPT âˆ— âˆ— âˆ— end\\_POSTSUPERSCRIPT\n\nâ€“\n\n+0.035âˆ—âˆ—âˆ—superscript0.035absent+0.035^{\\*\\*\\*}\\+ 0.035 start\\_POSTSUPERSCRIPT âˆ— âˆ— âˆ— end\\_POSTSUPERSCRIPT\n\nModified-GRPO\n\n0.197\n\n0.0498\n\nâˆ’0.017âˆ—âˆ—âˆ—superscript0.017absent\\-0.017^{\\*\\*\\*}\\- 0.017 start\\_POSTSUPERSCRIPT âˆ— âˆ— âˆ— end\\_POSTSUPERSCRIPT\n\n+0.00020.0002+0.0002\\+ 0.0002\n\n+0.035âˆ—âˆ—âˆ—superscript0.035absent+0.035^{\\*\\*\\*}\\+ 0.035 start\\_POSTSUPERSCRIPT âˆ— âˆ— âˆ— end\\_POSTSUPERSCRIPT\n\nDeepSeek-R1 14B\n\n0.214\n\n0.118\n\nâ€“\n\nâ€“\n\nâ€“\n\nSoft-Brier and ECE are means across questions; lower is better. Differences are (model â€“ reference). Stars mark two-sided pğ‘pitalic\\_p\\-values for the corresponding pair-wise tests (âˆ—\\*âˆ—<0.05absent0.05<0.05< 0.05, âˆ—âˆ—\\*\\*âˆ— âˆ—<0.01absent0.01<0.01< 0.01, âˆ—âˆ—âˆ—\\*\\*\\*âˆ— âˆ— âˆ—<0.001absent0.001<0.001< 0.001).\n\nRelative to the base model, ReMax lowers Brier by 0.0210.0210.0210.021 points (p<0.001ğ‘0.001p<0.001italic\\_p < 0.001) and reduces ECE by roughly a third (âˆ’0.0760.076\\-0.076\\- 0.076, p<0.001ğ‘0.001p<0.001italic\\_p < 0.001). Its Brier is statistically indistinguishable from o1 (+0.0030.003+0.003\\+ 0.003, p\\=0.23ğ‘0.23p=0.23italic\\_p = 0.23), while its calibration is markedly better (âˆ’0.0470.047\\-0.047\\- 0.047 ECE, p<0.001ğ‘0.001p<0.001italic\\_p < 0.001) and essentially matches Polymarket (+0.000090.00009+0.00009\\+ 0.00009 ECE, p\\=0.99ğ‘0.99p=0.99italic\\_p = 0.99). Modified-GRPO also improves substantially over the base model (âˆ’0.0170.017\\-0.017\\- 0.017 Brier, p<0.001ğ‘0.001p<0.001italic\\_p < 0.001; âˆ’0.0680.068\\-0.068\\- 0.068 ECE, p<0.001ğ‘0.001p<0.001italic\\_p < 0.001), but remains slightly worse than ReMax (Brier gap +0.00370.0037+0.0037\\+ 0.0037, p\\=0.024ğ‘0.024p=0.024italic\\_p = 0.024) and shows no reliable difference to o1 (Brier gap +0.00020.0002+0.0002\\+ 0.0002, p\\=0.95ğ‘0.95p=0.95italic\\_p = 0.95; ECE gap +0.00740.0074+0.0074\\+ 0.0074, p\\=0.23ğ‘0.23p=0.23italic\\_p = 0.23). Both learning schemes still trail the real-money market in Brier (ReMax +0.0310.031+0.031\\+ 0.031, Mod-GRPO +0.0350.035+0.035\\+ 0.035; p<0.001ğ‘0.001p<0.001italic\\_p < 0.001), yet ReMax shows very strong calibration.\n\n### 3.3 Hypothetical Trading Evaluation\n\nTo evaluate our forecasting algorithms in a more direct test, we convert every probability into a hypothetical one-share trade against the contemporaneous Polymarket price. For each resolved contract with non-zero volume we compare the modelâ€™s probability pğ‘pitalic\\_p with the last quoted market price mğ‘šmitalic\\_m. If p\\>mğ‘ğ‘šp>mitalic\\_p > italic\\_m the strategy buys one $1 long share at m+0.01ğ‘š0.01m+0.01italic\\_m + 0.01; if p<mğ‘ğ‘šp<mitalic\\_p < italic\\_m it shorts one share at (1âˆ’m)+0.011ğ‘š0.01(1-m)+0.01( 1 - italic\\_m ) + 0.01; exact ties are broken at random. The added cent approximates fees and slippage. Each trade therefore has a known entry cost cğ‘citalic\\_c, an expected value under the modelâ€™s belief (pğ‘pitalic\\_p for longs, 1âˆ’p1ğ‘1-p1 - italic\\_p for shorts), and a realised value vâˆˆ{0,1}ğ‘£01v\\\\!\\\\in\\\\!\\\\{0,1\\\\}italic\\_v âˆˆ { 0 , 1 } at resolution. We evaluate four models: ReMax Ensemble-7, Modified-GRPO, the untuned DeepSeek-R1 base model, and the frontier benchmark OpenAIo1. Profits vâˆ’cğ‘£ğ‘v-citalic\\_v - italic\\_c are aggregated in descending order of expected edge to yield cumulative-profit curves, and we report the total return under three bet-selection rules: trading until the edge drops below (i) the modelâ€™s own expected calibration error (Edge > ECE), (ii) zero after fees (Edge > 0), and (iii) across all markets. An intercept-only regression of per-trade profit gives the mean return and its 95%percent9595\\\\%95 % confidence interval.\n\nFigure [2](https://arxiv.org/html/2505.17989v1#S3.F2 \"Figure 2 â€£ 3.3 Hypothetical Trading Evaluation â€£ 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\") visualises how each model converts forecast skill into cash. The left panel shows cumulative realised profit as we step through the 3132 Polymarket questions in descending order of expected edge; the right panel condenses the final take under the three bet-selection rules.\n\n![Refer to caption](extracted/6467676/trading.png)\n\nFigure 2: Left: cumulative realised profit (USD) as each model sequentially places one-share trades, ranked by ex-ante expected edge. Solid disks mark the last trade with Edge > ECE; open rings mark the last trade with Edge > 0. Right: total profit under the three bet-selection rules. Truncating the strategy at the calibration threshold (Edge > ECE) retains almost the entire upside while avoiding the loss-making tail.\n\n##### Total profit differences\n\nIn the high-edge analysis (Edge \\>\\>\\> ECE) ReMax secures the largest absolute return, netting $127currency-dollar127\\\\$127$ 127, followed by Mod-GRPO with $111currency-dollar111\\\\$111$ 111, OpenAI o1 with $92currency-dollar92\\\\$92$ 92 and DeepSeekâ€“R1 14B with $72currency-dollar72\\\\$72$ 72. Uncertainty was quantified with a _question-level paired bootstrap_ (9999 replicates): each replicate resampled the set of Polymarket questions with replacement, kept the four modelsâ€™ profits for every question together (thereby preserving their correlation), summed those profits to obtain a total for each model, and then formed percentile 95 % confidence intervals; two-sided pğ‘pitalic\\_p\\-values were obtained by centring the bootstrap differences at zero. On this analysis, ReMax out-earns o1 by +$35.6currency-dollar35.6+\\\\$35.6\\+ $ 35.6 (95 % CI +$2.0currency-dollar2.0+\\\\$2.0\\+ $ 2.0 to +$68.6currency-dollar68.6+\\\\$68.6\\+ $ 68.6, p\\=0.037ğ‘0.037p=0.037italic\\_p = 0.037) and the DeepSeek base model by +$55.1currency-dollar55.1+\\\\$55.1\\+ $ 55.1 (CI +$23.3currency-dollar23.3+\\\\$23.3\\+ $ 23.3 to +$86.7currency-dollar86.7+\\\\$86.7\\+ $ 86.7, p\\=0.001ğ‘0.001p=0.001italic\\_p = 0.001), while its advantage over Mod-GRPO, +$16.9currency-dollar16.9+\\\\$16.9\\+ $ 16.9 (CI âˆ’$8.4currency-dollar8.4\\-\\\\$8.4\\- $ 8.4 to +$42.3currency-dollar42.3+\\\\$42.3\\+ $ 42.3, p\\=0.19ğ‘0.19p=0.19italic\\_p = 0.19), is not statistically significant. Mod-GRPO itself exceeds the DeepSeek base model by +$38.3currency-dollar38.3+\\\\$38.3\\+ $ 38.3 (CI +$2.7currency-dollar2.7+\\\\$2.7\\+ $ 2.7 to +$73.2currency-dollar73.2+\\\\$73.2\\+ $ 73.2, p\\=0.035ğ‘0.035p=0.035italic\\_p = 0.035), whereas the Mod-GRPO vs.o1 gap (+$18.7currency-dollar18.7+\\\\$18.7\\+ $ 18.7, CI âˆ’$18.7currency-dollar18.7\\-\\\\$18.7\\- $ 18.7 to +$56.4currency-dollar56.4+\\\\$56.4\\+ $ 56.4, p\\=0.33ğ‘0.33p=0.33italic\\_p = 0.33) and the o1 vs.DeepSeek base model gap (+$19.6currency-dollar19.6+\\\\$19.6\\+ $ 19.6, CI âˆ’$8.8currency-dollar8.8\\-\\\\$8.8\\- $ 8.8 to +$47.4currency-dollar47.4+\\\\$47.4\\+ $ 47.4, p\\=0.17ğ‘0.17p=0.17italic\\_p = 0.17) remain inconclusive. Overall, ReMax clearly outperforms both o1 and DeepSeek, Mod-GRPO also bests DeepSeek, and the remaining contrasts are indistinguishable within sampling error.\n\n##### Per-trade profit differences.\n\nWhen trading only on high signals (Edge \\>\\>\\> ECE), the models keep roughly 60-80% of the wagers that show any positive edge (ReMax 2193/2931, Modâ€“GRPO 2353/2985, o1 1718/2935, DeepSeek 1820/3013) yet harvest almost the entire per-trade profit. In this band ReMax averages 5.8 Â¢ per trade (95 % CI 4.1âˆ’7.54.17.54.1\\\\!-\\\\!7.54.1 - 7.5 Â¢) for a total of $127currency-dollar127\\\\$127$ 127; Modâ€“GRPO records 4.7 Â¢ \\[3.1,6.3\\]3.16.3\\[3.1,\\\\,6.3\\]\\[ 3.1 , 6.3 \\] and $111currency-dollar111\\\\$111$ 111; o1 5.4 Â¢ \\[3.5,7.3\\]3.57.3\\[3.5,\\\\,7.3\\]\\[ 3.5 , 7.3 \\] and $92currency-dollar92\\\\$92$ 92; DeepSeek 4.0 Â¢ \\[2.2,5.7\\]2.25.7\\[2.2,\\\\,5.7\\]\\[ 2.2 , 5.7 \\] and $72currency-dollar72\\\\$72$ 72. Pair-wise Welch tests on these per-trade means find no significant differences among ReMax, o1 and Modâ€“GRPO (largest gap = 1.1 Â¢, p\\=.35ğ‘.35p=.35italic\\_p = .35), indicating that their high-edge per-trade performance is statistically indistinguishable; ReMaxâ€™s 1.8 Â¢ advantage over DeepSeek is also not significant (p\\=.14ğ‘.14p=.14italic\\_p = .14).\n\n##### Practical takeaway.\n\nApplying a simple calibration filter, placing a bet only when the modelâ€™s predicted edge exceeds its own ECE, retains about 80% of the trading opportunities yet harvests at least 95% of the attainable profit. Within this high-edge regime, ReMax stands out as the top earner, beating both o1 and the DeepSeek base model in total hypothetical profit. Modâ€“GRPO and o1 deliver broadly comparable returns, and all three fine-tuned models comfortably exceed the untuned DeepSeek baseline. In short, calibration gating captures most of the dollars on the table, and the ReMax ensemble, due to its strong calibration, is the best of our models to deploy under that rule.\n\nFigure [3](https://arxiv.org/html/2505.17989v1#S3.F3 \"Figure 3 â€£ Practical takeaway. â€£ 3.3 Hypothetical Trading Evaluation â€£ 3 Results â€£ Outcome-based Reinforcement Learning to Predict the Future\") relates Polymarketâ€™s own confidence to the ex-post betting edge of the ReMax Ensemble-7. The horizontal axis spans 50%percent5050\\\\,\\\\%50 % (complete uncertainty) to 100%percent100100\\\\,\\\\%100 % (full certainty). The vertical axis shows the modelâ€™s mean excess win probability in percentage points (pp), with a dashed zero line marking break-even. We fit a penalised cubic-regression spline via mgcv::gam (REML, effective dfâ‰ˆ2.9ğ‘‘ğ‘“2.9df\\\\approx 2.9italic\\_d italic\\_f â‰ˆ 2.9) and plot it as a solid curve, while the shaded area denotes the pointwise 95%percent9595\\\\,\\\\%95 % confidence band. Tick labels above zero carry explicit plus signs, and axes are clipped to the empirically populated rectangle \\[50,100\\]Ã—\\[âˆ’5,15\\]50100515\\[50,100\\]\\\\times\\[-5,15\\]\\[ 50 , 100 \\] Ã— \\[ - 5 , 15 \\].\n\n![Refer to caption](extracted/6467676/improvement_by_market_confidence.png)\n\nFigure 3: Average betting edge of ReMax Ensemble-7 versus Polymarket across market-confidence levels. Solid line shows the GAM fit; shaded band indicates the 95%percent9595\\\\,\\\\%95 % co

... (truncated)
```
