# Tool Output: tool_c754876f5001Yv5wWcsFLzY1Su
**Date**: 2026-02-19 09:43:29 UTC
**Size**: 139,311 bytes

```
Detailed Results:

Title: [2505.20096] MA-RAG: Multi-Agent Retrieval-Augmented ... - arXiv.org
URL: https://arxiv.org/abs/2505.20096
Content: # Title:MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning. Authors:Thang Nguyen, Peter Chin, Yu-Wing Tai. View a PDF of the paper titled MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning, by Thang Nguyen and 2 other authors. View a PDF of the paper titled MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning, by Thang Nguyen and 2 other authors. > Abstract:We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on end-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates a collaborative set of specialized AI agents: Planner, Step Definer, Extractor, and QA Agents, each responsible for a distinct stage of the RAG pipeline. Our results highlight the effectiveness of collaborative, modular reasoning in retrieval-augmented systems: MA-RAG not only improves answer accuracy and robustness but also provides interpretable intermediate reasoning steps, establishing a new paradigm for efficient and reliable multi-agent RAG. |  | (or  arXiv:2505.20096v2 [cs.CL] for this version) |. # Bibliographic and Citation Tools.
Raw Content: We gratefully acknowledge support from  
the Simons Foundation, [Stockholm University](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)

> [cs](/list/cs/recent) > arXiv:2505.20096



# Computer Science > Computation and Language

**arXiv:2505.20096** (cs)

[Submitted on 26 May 2025 ([v1](https://arxiv.org/abs/2505.20096v1)), last revised 11 Oct 2025 (this version, v2)]

# Title:MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning

Authors:[Thang Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen,+T), [Peter Chin](https://arxiv.org/search/cs?searchtype=author&query=Chin,+P), [Yu-Wing Tai](https://arxiv.org/search/cs?searchtype=author&query=Tai,+Y)

View a PDF of the paper titled MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning, by Thang Nguyen and 2 other authors

[View PDF](/pdf/2505.20096) [HTML (experimental)](https://arxiv.org/html/2505.20096v2)
> Abstract:We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on end-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates a collaborative set of specialized AI agents: Planner, Step Definer, Extractor, and QA Agents, each responsible for a distinct stage of the RAG pipeline. By decomposing tasks into subtasks such as query disambiguation, evidence extraction, and answer synthesis, and enabling agents to communicate intermediate reasoning via chain-of-thought prompting, MA-RAG progressively refines retrieval and synthesis while maintaining modular interpretability. Extensive experiments on multi-hop and ambiguous QA benchmarks, including NQ, HotpotQA, 2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly outperforms standalone LLMs and existing RAG methods across all model scales. Notably, even a small LLaMA3-8B model equipped with MA-RAG surpasses larger standalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new state-of-the-art results on challenging multi-hop datasets. Ablation studies reveal that both the planner and extractor agents are critical for multi-hop reasoning, and that high-capacity models are especially important for the QA agent to synthesize answers effectively. Beyond general-domain QA, MA-RAG generalizes to specialized domains such as medical QA, achieving competitive performance against domain-specific models without any domain-specific fine-tuning. Our results highlight the effectiveness of collaborative, modular reasoning in retrieval-augmented systems: MA-RAG not only improves answer accuracy and robustness but also provides interpretable intermediate reasoning steps, establishing a new paradigm for efficient and reliable multi-agent RAG.

|  |  |
| --- | --- |
| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI) |
| Cite as: | [arXiv:2505.20096](https://arxiv.org/abs/2505.20096) [cs.CL] |
|  | (or  [arXiv:2505.20096v2](https://arxiv.org/abs/2505.20096v2) [cs.CL] for this version) |
|  | <https://doi.org/10.48550/arXiv.2505.20096> arXiv-issued DOI via DataCite |

## Submission history

From: Thang Nguyen [[view email](/show-email/e6026b1d/2505.20096)]   
 **[[v1]](/abs/2505.20096v1)** Mon, 26 May 2025 15:05:18 UTC (458 KB)  
 **[v2]** Sat, 11 Oct 2025 16:19:57 UTC (305 KB)

Full-text links:

## Access Paper:

View a PDF of the paper titled MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning, by Thang Nguyen and 2 other authors

* [View PDF](/pdf/2505.20096)
* [HTML (experimental)](https://arxiv.org/html/2505.20096v2)
* [TeX Source](/src/2505.20096)

[view license](http://creativecommons.org/licenses/by/4.0/ "Rights to this article")

Current browse context:

cs.CL

[<prev](/prevnext?id=2505.20096&function=prev&context=cs.CL "previous in cs.CL (accesskey p)")   |   [next>](/prevnext?id=2505.20096&function=next&context=cs.CL "next in cs.CL (accesskey n)")

[new](/list/cs.CL/new)  |  [recent](/list/cs.CL/recent)  | [2025-05](/list/cs.CL/2025-05)

Change to browse by:

[cs](/abs/2505.20096?context=cs)  
 [cs.AI](/abs/2505.20096?context=cs.AI)

### References & Citations

* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2505.20096)
* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2505.20096)
* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2505.20096)

export BibTeX citation Loading...

## BibTeX formatted citation

×

Data provided by:

### Bookmark

# Bibliographic and Citation Tools

Bibliographic Explorer *([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*

Connected Papers *([What is Connected Papers?](https://www.connectedpapers.com/about))*

Litmaps *([What is Litmaps?](https://www.litmaps.co/))*

scite Smart Citations *([What are Smart Citations?](https://www.scite.ai/))*

# Code, Data and Media Associated with this Article

alphaXiv *([What is alphaXiv?](https://alphaxiv.org/))*

CatalyzeX Code Finder for Papers *([What is CatalyzeX?](https://www.catalyzex.com))*

DagsHub *([What is DagsHub?](https://dagshub.com/))*

Gotit.pub *([What is GotitPub?](http://gotit.pub/faq))*

Hugging Face *([What is Huggingface?](https://huggingface.co/huggingface))*

Papers with Code *([What is Papers with Code?](https://paperswithcode.com/))*

ScienceCast *([What is ScienceCast?](https://sciencecast.org/welcome))*

# Demos

Replicate *([What is Replicate?](https://replicate.com/docs/arxiv/about))*

Hugging Face Spaces *([What is Spaces?](https://huggingface.co/docs/hub/spaces))*

TXYZ.AI *([What is TXYZ.AI?](https://txyz.ai))*

# Recommenders and Search Tools

Influence Flower *([What are Influence Flowers?](https://influencemap.cmlab.dev/))*

CORE Recommender *([What is CORE?](https://core.ac.uk/services/recommender))*

* Author
* Venue
* Institution
* Topic

# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](/auth/show-endorsers/2505.20096) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))

Title: [2412.05838] A Collaborative Multi-Agent Approach to Retrieval ...
URL: https://arxiv.org/abs/2412.05838
Content: # Title:A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data. View a PDF of the paper titled A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data, by Aniruddha Salve and 4 other authors. Traditional RAG systems typically use a single-agent architecture to handle query generation, data retrieval, and response synthesis. Specialized agents, each optimized for a specific data source, handle query generation for relational, NoSQL, and document-based systems. The proposed system is scalable and adaptable, making it ideal for generative AI workflows that require integration with diverse, dynamic, or private data sources. By leveraging specialized agents and a modular execution environment, the system provides an efficient and robust solution for handling complex, heterogeneous data environments in generative AI applications. This preprint introduces a multi-agent framework for Retrieval-Augmented Generation (RAG), enhancing Large Language Models (LLMs) for efficient integration of diverse data sources. View a PDF of the paper titled A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data, by Aniruddha Salve and 4 other authors. # Bibliographic and Citation Tools.
Raw Content: We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)

> [cs](/list/cs/recent) > arXiv:2412.05838



# Computer Science > Artificial Intelligence

**arXiv:2412.05838** (cs)

[Submitted on 8 Dec 2024]

# Title:A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data

Authors:[Aniruddha Salve](https://arxiv.org/search/cs?searchtype=author&query=Salve,+A), [Saba Attar](https://arxiv.org/search/cs?searchtype=author&query=Attar,+S), [Mahesh Deshmukh](https://arxiv.org/search/cs?searchtype=author&query=Deshmukh,+M), [Sayali Shivpuje](https://arxiv.org/search/cs?searchtype=author&query=Shivpuje,+S), [Arnab Mitra Utsab](https://arxiv.org/search/cs?searchtype=author&query=Utsab,+A+M)

View a PDF of the paper titled A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data, by Aniruddha Salve and 4 other authors

[View PDF](/pdf/2412.05838) [HTML (experimental)](https://arxiv.org/html/2412.05838v1)
> Abstract:Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external, domain-specific data into the generative process. While LLMs are highly capable, they often rely on static, pre-trained datasets, limiting their ability to integrate dynamic or private data. Traditional RAG systems typically use a single-agent architecture to handle query generation, data retrieval, and response synthesis. However, this approach becomes inefficient when dealing with diverse data sources, such as relational databases, document stores, and graph databases, often leading to performance bottlenecks and reduced accuracy. This paper proposes a multi-agent RAG system to address these limitations. Specialized agents, each optimized for a specific data source, handle query generation for relational, NoSQL, and document-based systems. These agents collaborate within a modular framework, with query execution delegated to an environment designed for compatibility across various database types. This distributed approach enhances query efficiency, reduces token overhead, and improves response accuracy by ensuring that each agent focuses on its specialized task. The proposed system is scalable and adaptable, making it ideal for generative AI workflows that require integration with diverse, dynamic, or private data sources. By leveraging specialized agents and a modular execution environment, the system provides an efficient and robust solution for handling complex, heterogeneous data environments in generative AI applications.

|  |  |
| --- | --- |
| Comments: | 16 pages, 3 figures. This preprint introduces a multi-agent framework for Retrieval-Augmented Generation (RAG), enhancing Large Language Models (LLMs) for efficient integration of diverse data sources. Relevant for researchers in AI, ML, generative AI, and database systems |
| Subjects: | Artificial Intelligence (cs.AI) |
| Cite as: | [arXiv:2412.05838](https://arxiv.org/abs/2412.05838) [cs.AI] |
|  | (or  [arXiv:2412.05838v1](https://arxiv.org/abs/2412.05838v1) [cs.AI] for this version) |
|  | <https://doi.org/10.48550/arXiv.2412.05838> arXiv-issued DOI via DataCite |

## Submission history

From: Arnab Mitra Utsab [[view email](/show-email/23d1fb45/2412.05838)]   
 **[v1]** Sun, 8 Dec 2024 07:18:19 UTC (763 KB)

Full-text links:

## Access Paper:

View a PDF of the paper titled A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data, by Aniruddha Salve and 4 other authors

* [View PDF](/pdf/2412.05838)
* [HTML (experimental)](https://arxiv.org/html/2412.05838v1)
* [TeX Source](/src/2412.05838)

[view license](http://creativecommons.org/licenses/by/4.0/ "Rights to this article")

Current browse context:

cs.AI

[<prev](/prevnext?id=2412.05838&function=prev&context=cs.AI "previous in cs.AI (accesskey p)")   |   [next>](/prevnext?id=2412.05838&function=next&context=cs.AI "next in cs.AI (accesskey n)")

[new](/list/cs.AI/new)  |  [recent](/list/cs.AI/recent)  | [2024-12](/list/cs.AI/2024-12)

Change to browse by:

[cs](/abs/2412.05838?context=cs)

### References & Citations

* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2412.05838)
* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2412.05838)
* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2412.05838)

export BibTeX citation Loading...

## BibTeX formatted citation

×

Data provided by:

### Bookmark

# Bibliographic and Citation Tools

Bibliographic Explorer *([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*

Connected Papers *([What is Connected Papers?](https://www.connectedpapers.com/about))*

Litmaps *([What is Litmaps?](https://www.litmaps.co/))*

scite Smart Citations *([What are Smart Citations?](https://www.scite.ai/))*

# Code, Data and Media Associated with this Article

alphaXiv *([What is alphaXiv?](https://alphaxiv.org/))*

CatalyzeX Code Finder for Papers *([What is CatalyzeX?](https://www.catalyzex.com))*

DagsHub *([What is DagsHub?](https://dagshub.com/))*

Gotit.pub *([What is GotitPub?](http://gotit.pub/faq))*

Hugging Face *([What is Huggingface?](https://huggingface.co/huggingface))*

Papers with Code *([What is Papers with Code?](https://paperswithcode.com/))*

ScienceCast *([What is ScienceCast?](https://sciencecast.org/welcome))*

# Demos

Replicate *([What is Replicate?](https://replicate.com/docs/arxiv/about))*

Hugging Face Spaces *([What is Spaces?](https://huggingface.co/docs/hub/spaces))*

TXYZ.AI *([What is TXYZ.AI?](https://txyz.ai))*

# Recommenders and Search Tools

Influence Flower *([What are Influence Flowers?](https://influencemap.cmlab.dev/))*

CORE Recommender *([What is CORE?](https://core.ac.uk/services/recommender))*

* Author
* Venue
* Institution
* Topic

# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](/auth/show-endorsers/2412.05838) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))

Title: HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented ...
URL: https://arxiv.org/html/2504.12330v1
Content: The framework is composed of three-tiered architecture with specialized agents: a Decomposition Agent that dissects complex queries into contextually coherent sub-tasks via semantic-aware query rewriting and schema-guided context augmentation; Multi-source Retrieval Agents that carry out parallel, modality-specific retrieval using plug-and-play modules designed for vector, graph, and web-based databases; and a Decision Agent that uses consistency voting to integrate multi-source answers and resolve discrepancies in retrieval results through Expert Model Refinement. While text-based retrieval-augmented generation (RAG) systems have demonstrated robust performance in processing linguistic information (Naveed etal., 2023), their inability to handle visual content has spurred the development of image-based RAG approaches (Hu etal., 2024; Chen etal., 2024; Luo etal., 2024). Initially, text-based RAG systems integrated Large Language Models (LLMs) with external textual knowledge, improving performance in question answering by retrieving relevant text fragments (Izacard etal., 2022; Asai etal., 2023; Zhang etal., 2024).
Raw Content: # HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation

Pei Liu1,2, Xin Liu2, Ruoyu Yao2, Junming Liu1, Siyuan Meng1, Ding Wang1∗, Jun Ma23∗  1Shanghai Artificial Intelligence Laboratory 2The Hong Kong University of Science and Technology (Guangzhou) 3The Hong Kong University of Science and Technology   
pliu061@connect.hkust-gz.edu.cnwangding@pjlab.org.cn jun.ma@ust.hk

###### Abstract.

While Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge, conventional single-agent RAG remains fundamentally limited in resolving complex queries demanding coordinated reasoning across heterogeneous data ecosystems. We present HM-RAG, a novel Hierarchical Multi-agent Multimodal RAG framework that pioneers collaborative intelligence for dynamic knowledge synthesis across structured, unstructured, and graph-based data. The framework is composed of three-tiered architecture with specialized agents: a Decomposition Agent that dissects complex queries into contextually coherent sub-tasks via semantic-aware query rewriting and schema-guided context augmentation; Multi-source Retrieval Agents that carry out parallel, modality-specific retrieval using plug-and-play modules designed for vector, graph, and web-based databases; and a Decision Agent that uses consistency voting to integrate multi-source answers and resolve discrepancies in retrieval results through Expert Model Refinement. This architecture attains comprehensive query understanding by combining textual, graph-relational, and web-derived evidence, resulting in a remarkable 12.95% improvement in answer accuracy and a 3.56% boost in question classification accuracy over baseline RAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG establishes state-of-the-art results in zero-shot settings on both datasets. Its modular architecture ensures seamless integration of new data modalities while maintaining strict data governance, marking a significant advancement in addressing the critical challenges of multimodal reasoning and knowledge synthesis in RAG systems. Code is available at <https://github.com/ocean-luna/HMRAG>.

Retrieval-Augmented Generation (RAG), Multimodal Representation, Multi-agent Systems, Multi-source RAG

††copyright: acmlicensed††journalyear: 2018††doi: XXXXXXX.XXXXXXX††isbn: 978-1-4503-XXXX-X/2018/06††submissionid: 500

## 1. Introduction

In an era defined by the rapid proliferation of data, the ability to efficiently retrieve relevant information from heterogeneous sources has emerged as a fundamental pillar of modern information systems (Fey etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib15)). Multimodal retrieval systems, which integrate text, images, vectorized data, and web-based content, are becoming indispensable across domains such as e-commerce, healthcare, and scientific research (Zhong and Mottin, [2023](https://arxiv.org/html/2504.12330v1#bib.bib60)). These systems enable the seamless navigation of diverse data types, empowering users to derive actionable insights across multiple modalities. However, despite remarkable progress in recent years, multimodal retrieval continues to present significant challenges. The complexity arises from the need to reconcile the diversity of query types, the heterogeneity of data formats, and the varying objectives of retrieval tasks, all of which demand sophisticated solutions to bridge the gap between data representation and user intent.

The evolution of retrieval technologies has historically centered on single-modal architectures, where queries and retrieval mechanisms operate within a single predefined modality (Lewis etal., [2020](https://arxiv.org/html/2504.12330v1#bib.bib34); Anand etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib4)). While text-based retrieval-augmented generation (RAG) systems have demonstrated robust performance in processing linguistic information (Naveed etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib44)), their inability to handle visual content has spurred the development of image-based RAG approaches (Hu etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib26); Chen etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib8); Luo etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib42)). However, current multimodal implementations face a critical bottleneck: Although image-based RAG systems excel at visual content processing, they often fail to establish coherent cross-modal correlations between visual elements and textual context. This limitation is particularly acute in multimodal question answering, where systems must integrate visual perception with textual semantics to generate contextually relevant responses.

Recently, graph-based retrieval frameworks have been proposed to enhance the modeling of textual interdependencies based on the construction of knowledge graphs, represented by GraphRAG (Edge etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib13)) and LightRAG (Guo etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib19)). These approaches are further extended to processing multimodal inputs (Liu etal., [2025a](https://arxiv.org/html/2504.12330v1#bib.bib38)), where graph structures are leveraged for the accurate capture of cross-modal relationships. Despite these advances, graph-based methods face an inherent trade-off: while they effectively capture high-level modality interactions, they often sacrifice fine-grained information fidelity. This becomes problematic in scenarios requiring precise textual segment retrieval, as the abstraction process inherent to graph modeling obscures granular textual details critical for nuanced analysis.

Meanwhile, another critical challenge has been noticed in reconciling the complementary strengths of different modalities (Gao etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib16); Khattab and Zaharia, [2020](https://arxiv.org/html/2504.12330v1#bib.bib32); Faysse etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib14)). Textual modalities excel at encoding granular semantic details and conceptual relationships, while visual modalities, by contrast, are capable of capturing spatial context and facilitating spatial relationship understanding. Current modality-specific systems (Lewis etal., [2020](https://arxiv.org/html/2504.12330v1#bib.bib34); Xia etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib55)) exhibit critical limitations in cross-modal synthesis, producing retrieval outcomes that are either overspecialized in textual precision or confined to visual pattern recognition. This modality isolation creates systemic vulnerabilities in heterogeneous data environments, where the absence of cross-modal alignment protocols risks critical information loss during retrieval operations. For instance, visual queries in text-centric systems fail to map conceptual questions to illustrative elements, while text-intensive inquiries in vision-oriented frameworks lack mechanisms for lexical disambiguation. These architectural gaps highlight the urgent need for frameworks that can harmonize granular semantic detail with cross-modal contextual coherence.

To address these challenges, we introduce Hierarchical Multi-Agent Retrieval-Augmented Generation (HM-RAG), a novel framework that enhances multimodal retrieval through coordinated multi-agent collaboration. HM-RAG employs a three-tiered architecture with specialized agents operating in the RAG pipelines. The Decomposition Agent analyzes query intent and dynamically rewrites requests to ensure cross-modal compatibility. The Multi-Source Retrieval Agent conducts parallel knowledge acquisition via lightweight multimodal retrievals across diverse data sources, including vectors, graphs, and web-based databases. Finally, the Decision Agent synthesizes and refines candidate responses using domain-specific verification strategies to ensure accuracy and coherence. This hierarchical design systematically orchestrates text-image evidence integration through structured agent interactions, enabling layered reasoning. Unlike conventional approaches, HM-RAG combines query decomposition, parallelized information retrieval, and expert-guided answer refinement to achieve efficient and contextually relevant responses. Our contributions are summarized as follows:

* •

  We propose a novel Modularized Hierarchical Framework that modularizes query processing into specialized agent-based components, and this facilitates scalable and efficient multimodal retrieval.
* •

  We enable Multi-source Plug-and-play Retrieval Integration, which offers seamless connectivity across diverse data sources. By efficiently routing queries to vector, graph, and web-based retrieval agents, our approach ensures flexibility and efficiency in handling heterogeneous data environments, streamlining complex information retrieval processes.
* •

  We employ Expert-guided Refinement processes to enhance response quality to ensure both operational efficiency and contextual precision through minimal expert oversight.
* •

  We demonstrate the effectiveness of HM-RAG through extensive experiments on benchmark datasets, and the results attain State-of-the-art Performance on the ScienceQA and CrisisMMD benchmarks.

## 2. Related Work

### 2.1. Retrieval-Augmented Generation

RAG systems have evolved significantly to enhance their multimodal reasoning capabilities (Lewis etal., [2020](https://arxiv.org/html/2504.12330v1#bib.bib34); Guu etal., [2020](https://arxiv.org/html/2504.12330v1#bib.bib21); Genesis and Keane, [2025](https://arxiv.org/html/2504.12330v1#bib.bib17); Şakar and Emekci, [2025](https://arxiv.org/html/2504.12330v1#bib.bib48)). Initially, text-based RAG systems integrated Large Language Models (LLMs) with external textual knowledge, improving performance in question answering by retrieving relevant text fragments (Izacard etal., [2022](https://arxiv.org/html/2504.12330v1#bib.bib28); Asai etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib5); Zhang etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib58)). However, as visually rich documents became more prevalent, the limitations of text-only systems became evident, prompting the development of image-based RAG approaches (Bag etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib6); Bonomo and Bianco, [2025](https://arxiv.org/html/2504.12330v1#bib.bib7); Riedler and Langer, [2024](https://arxiv.org/html/2504.12330v1#bib.bib47); Liu etal., [2025b](https://arxiv.org/html/2504.12330v1#bib.bib39)). While these methods aimed to retrieve visual content for Large Vision-Language Models (VLMs), they faced challenges in effectively integrating text and image modalities, as the retrieval processes were largely independent, hindering a deep understanding of their interrelationships.

To address these challenges, graph-based RAG systems emerged, leveraging structured knowledge representations to capture both inter-modal and intra-modal semantic relationships (Dong etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib10); Jeong, [2024a](https://arxiv.org/html/2504.12330v1#bib.bib29); Guo etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib19); Procko and Ochoa, [2024](https://arxiv.org/html/2504.12330v1#bib.bib45)). These systems utilize vector-space embeddings and topological relationships to model complex document structures, enabling the retrieval of semantically coherent contexts that go beyond simple text fragments (Wu etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib54); Edge etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib13); Mavromatis and Karypis, [2024](https://arxiv.org/html/2504.12330v1#bib.bib43)). Graph-based RAG systems are particularly effective in understanding relationships between text and images, as well as extracting relationships within the text itself (Liu etal., [2025a](https://arxiv.org/html/2504.12330v1#bib.bib38)). However, current RAG implementations often rely on single-source retrieval, limiting their ability to handle complex queries that require simultaneous processing of vector, graph, and web-based databases (Gupta etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib20)). This limitation is particularly significant in applications requiring private data retrieval and real-time updates, where the absence of integrated multi-source retrieval capabilities can lead to incomplete or outdated information. To fully leverage the strengths of each data modality and meet the demands of dynamic and heterogeneous data environments, RAG systems must evolve to support coordinated multi-source retrieval and synthesis.

### 2.2. Agents in RAG

RAG has become a key paradigm for knowledge-intensive tasks by integrating retrieval mechanisms with generative models, significantly enhancing language model capabilities. However, traditional RAG implementations often rely on static pipelines that struggle with multimodal query processing (Schick etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib49); Cheng etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib9)). Recent agent-based RAG architectures have addressed these limitations by improving system modularity and operational flexibility (Han etal., [2025](https://arxiv.org/html/2504.12330v1#bib.bib22); Jeong, [2024b](https://arxiv.org/html/2504.12330v1#bib.bib30); eAquino etal., [2025](https://arxiv.org/html/2504.12330v1#bib.bib12)). The agent-oriented approach breaks down query processing into specialized components like semantic parsing, cross-modal retrieval, and context-aware generation, allowing targeted optimization while maintaining overall adaptability. PaperQA (Lála etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib33)) exemplifies this by leveraging academic literature to generate evidence-based responses, reducing hallucinations in scientific applications.

Building on this, Active RAG methodologies like FLARE (Jiang etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib31)) introduce temporal dynamism through anticipatory retrieval, enhancing performance in extended text generation. Despite these advances, challenges in multimodal integration persist. Emerging Dynamic RAG approaches (Su etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib50); Toro etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib51)) propose entity-aware augmentation strategies to dynamically incorporate retrieved entity representations, addressing context window limitations while preserving semantic coherence. Our HM-RAG framework synthesizes these innovations through a hierarchical multi-agent architecture leveraging LLMs’ semantic comprehension. This design enables dynamic query adaptation and multimodal retrieval, providing an optimized solution for complex information retrieval and generation tasks across diverse data modalities. By integrating these advancements, HM-RAG addresses key challenges in multimodal reasoning and knowledge synthesis, paving the way for more robust and adaptable RAG systems.

## 3. Methodology

We introduce HM-RAG, a novel framework tackling complex challenges in RAG systems. As depicted in Figure [2](https://arxiv.org/html/2504.12330v1#S3.F2 "Figure 2 ‣ 3. Methodology ‣ HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation"), HM-RAG features an innovative multi-agent, multimodal architecture with specialized agents for information extraction and multi-source retrieval. Given a natural language question and a reference document , RAG retrieves semantically relevant content from , integrating it with generative language models to produce answers strictly grounded in . This approach advances multimodal question answering and multi-agent RAG capabilities. The subsequent sections provide a detailed exposition of HM-RAG’s architectural design. Through this systematic description, we elucidate the framework’s core mechanisms for effectively integrating and utilizing multimodal information and multi-source retrieval, ultimately leading to enhanced accuracy in RAG applications.

### 3.1. Multimodal Knowledge Pre-Processing

This section focuses on multimodal data processing, aiming to convert textual data and visual images into vector and graph database representations for enhanced retrieval operations. Our methodology employs VLMs to transcode visual information into textual representations, which are subsequently integrated with original text corpora to jointly construct vector and graph databases.

#### 3.1.1. Multimodal Textual Knowledge Generation

Conventional entity-centric approaches for multimodal knowledge extraction rely on predefined categorical boundaries, limiting their capacity to recognize novel visual concepts. We utilize the BLIP-2’s framework (Li etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib35)) to harness the open vocabulary potential of pretrained VLMs. Building upon the generalized vision to language conversion paradigm:

|  |  |
| --- | --- |
| (1) |  |

where visual encoder extracts features from input image and cross-modal alignment module bridges vision-language semantics. Our framework addresses the critical limitation of oversimplified machine-generated descriptions, particularly addressing BLIP-2’s over-condensed outputs that lack visual specificity, through contextual refinement mechanisms leveraging original textual data.

This process is divided into three synergistic phases. Hierarchical visual encoding via established architectures (He etal., [2016](https://arxiv.org/html/2504.12330v1#bib.bib23); Liu etal., [2021](https://arxiv.org/html/2504.12330v1#bib.bib40); Dosovitskiy etal., [2020](https://arxiv.org/html/2504.12330v1#bib.bib11)) to generate patch embeddings . Cross-modal interaction where learnable queries attend to visual features through scaled dot product attention, dynamically weighting spatial semantic correlations. Context-aware text generation that fuses latent text features from prior descriptions with cross-modal representations for autoregressive decoding. Contextual refinement during this phase enhances semantic alignment, achieving measurable reductions in descriptive ambiguity and lexical sparsity for the final output .

The resultant multimodal textual knowledge base is subsequently formed through the systematic integration of original textual inputs with generated textualizations.

|  |  |
| --- | --- |
| (2) |  |

where corresponds to the source textual corpus and represents the multimodal textual aggregation formed through heterogeneous fusion processes.

#### 3.1.2. Multimodal Knowledge Graphs Construction

We establish multimodal knowledge graphs (MMKGs) by synergizing VLM-enhanced descriptions with LLM-based structural reasoning. Building upon the refined visual descriptions generated by VLMs, optionally fused with external textual knowledge , we employ the LightRAG framework (Guo etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib19)) for efficient multi-hop reasoning and dynamic knowledge integration:

|  |  |
| --- | --- |
| (3) |  |

LightRAG processes multimodal inputs through a hybrid extraction strategy. Entity-Relation Extraction: a specialized function decomposes inputs into entities and relation triplets , where represent head/tail entities and denotes relations. Dual-level Reasoning Augmentation: Dual-scale retrieval mechanisms dynamically fetch relevant triplets during inference; global retrieval identifies thematic clusters while local extraction focuses on entity-specific connections.

The constructed MMKG formalizes knowledge as triplets , where entities encompass both visual concepts from and textual knowledge from . Crucially, visual data storage locations are embedded during graph construction, enabling cross-modal grounding. This architecture establishes a bidirectional knowledge enhancement framework: language models achieve visual-contextualized reasoning through visual-semantic relationships embedded in , and vision-language models dynamically update knowledge embeddings via continuous multimodal integration, effectively mitigating hallucination probabilities through representation consistency constraints.

### 3.2. Decomposition Agent for Multi-intent Queries

The Decomposition Agent is a pivotal component of the proposed framework, designed to break down complex, multi-intent user queries into coherent and executable sub-tasks. This agent addresses a critical limitation of traditional systems, which often struggle to process compound queries requiring joint reasoning across multiple data sources. By leveraging a hierarchical parsing mechanism, the Decomposition Agent identifies the underlying structure of user queries and decomposes them into atomic units, with each targeting a specific data modality or retrieval task.

The proposed framework operates in two stages, both driven by task-specific LLM-prompting strategies. Decomposition Necessity Judgment. The agent first determines whether the input question contains multiple intents using a binary decision prompt that instructs the LLM to classify it as single-intent or multi-intent. If the output is multi-intent, proceeds to decomposition. Otherwise, return question directly. Intent Decomposition. The LLM decomposes into candidate sub-questions using a structured prompt: ”Decompose the reasoning steps of the original question into 2 to 3 simply and logically connected sub-questions based on its intent while retaining keywords from the original question.” inspired by (Li etal., [2025](https://arxiv.org/html/2504.12330v1#bib.bib36)).

### 3.3. Multi-source Plug-and-Play Retrieval Agents

We propose a modular multi-agent retrieval framework that dynamically composes heterogeneous multimodal search strategies through standardized interfaces. By decoupling retrieval functionalities into three specialized agents—vector-based retrieval agent, graph-based retrieval agent, and web-based retrieval agent—the system achieves domain-agnostic adaptability while ensuring interoperability across diverse search scenarios. Each agent adheres to unified communication protocols, enabling seamless integration of vector semantic search, graph topological exploration, and real-time web retrieval capabilities. This design allows each retrieval agent to function as a plug-and-play component, ensuring that they can be easily integrated or replaced without affecting the overall system performance. This modularity not only enhances flexibility but also maintains task-specific optimization objectives, making the framework highly adaptable to various applications and data modalities.

#### 3.3.1. Vector-based Retrieval Agent for Fine-Grained Information

This agent leverages a naive retrieval architecture (Guo etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib19)) to search unstructured textual corpora efficiently. Given the user query , the system first computes its semantic embedding using an encoder :

|  |  |
| --- | --- |
| (4) |  |

where represents the query’s embedding in a -dimensional vector space.

Next, the system computes the semantic similarity between the query embedding and all document embeddings using cosine similarity:

|  |  |
| --- | --- |
| (5) |  |

where , with being the total number of documents. The similarity score quantifies how closely each document aligns with the query, forming the basis for ranking retrieved documents.

Based on the similarity scores, the system retrieves the top- most relevant documents:

|  |  |
| --- | --- |
| (6) |  |

where denotes the set of top- retrieved contexts, ensuring that only the most relevant information is used for subsequent processing.

Subsequently, the language model generates answers conditioned on retrieved contexts through constrained decoding:

|  |  |
| --- | --- |
| (7) |  |

where represents the generation process, which concatenates the query , retrieved contexts , and additional contextual information to produce the final answer.

Specifically, the conditional probability of generating a token sequence y given the query q and retrieved contexts is modeled as:

|  |  |
| --- | --- |
| (8) |  |

where denotes the conditional probability of a token in the auto-regressive generation process of a language model, ensuring that the generated answer is contextually coherent.

Furthermore, the attention mechanism explicitly incorporates retrieved content into the generation process:

|  |  |
| --- | --- |
| (9) |  |

where stacks the embeddings of retrieved chunks, and concatenates the query embedding with the retrieved chunk embeddings, enhancing the model’s ability to focus on relevant information. To ensure the reliability of the generated answers, constraints enforce top- and a temperature of 0, ensuring deterministic decoding based on the highest probability tokens. This minimizes the risk of hallucination and ensures factual accuracy.

#### 3.3.2. Graph-based Retrieval Agent for Relational Information

This agent leverages LightRAG’s graph traversal capabilities to resolve multi-hop semantic queries over MMKGs (Guo etal., [2024](https://arxiv.org/html/2504.12330v1#bib.bib19)). Given an input query , the agent constructs a context-aware subgraph by dynamically retrieving entities and relations through the joint attention mechanism of LightRAG. The subgraph is defined as:

|  |  |
| --- | --- |
| (10) |  |

where computes relevance scores by aligning query embeddings with graph triplet representations through cross-modal attention, ensuring that only highly relevant triplets are included in the subgraph.

To efficiently address complex queries, the agent employs a hierarchical search strategy that balances efficiency and comprehensiveness. First, the agent prioritizes local 1-hop neighbors of query-relevant entities using relation-specific attention weights. This ensures that directly connected entities and relations are retrieved first, providing a foundation for further exploration. Next, the agent expands the search globally by identifying cross-modal paths through iterative message passing. This allows the agent to explore deeper semantic relationships beyond immediate neighbors, enhancing the richness of the retrieved information.

Furthermore, the framework is a dual-level retrieval framework that integrates graph-structured knowledge with vector representations through a three-phase retrieval process. First, the framework performs semantic decomposition of the input query to derive local keywords and global keywords . This step captures both fine-grained and high-level semantic information. Second, the framework executes hybrid graph-vector matching. An optimized vector database aligns with entity attributes while mapping to relational patterns in the knowledge graph . This hybrid approach ensures that both explicit entity attributes and latent relational semantics are considered.

Finally, to enhance retrieval completeness, the framework performs higher-order context expansion. The retrieved subgraph is expanded to include one-hop neighbors of both retrieved nodes and edges:

|  |  |
| --- | --- |
| (11) |  |

where and denote the one-hop neighbors of retrieved nodes and edges, respectively. This step ensures that the retrieved subgraph retains structural integrity while capturing broader contextual relationships. The final answer is generated using with a lightweight LLM.

#### 3.3.3. Web-based Retrieval Agent for Real-Time Information

The web retrieval component serves as a critical bridge between information retrieval and natural language generation, significantly enhancing the semantic fidelity and factual grounding of generated text. Our work utilizes the Google Serper API. The system acquires knowledge through parameterized API requests to Google’s search engine. For an input query , the retrieval process is formalized as:

|  |  |
| --- | --- |
| (12) |  |

where specifies search configuration parameters. We adopt the setting that . The API returns structured results , each containing a title, a snippet, a URL, and positional ranking metadata.

The Google Serper framework demonstrates particular efficacy in real-world deployment scenarios through three principal operational modalities, each addressing the critical requirements of modern knowledge-aware systems. First, the real-time fact verification module computes factual validity scores through neural memory interrogation. Second, the attribution-aware generation protocol ensures traceability through dual-phase attention routing. Third, the adaptive query expansion mechanism addresses vocabulary mismatch through differential term weighting.

### 3.4. Decision Agent for Multi-answer Refinement

Consistency Voting. The framework evaluates the semantic agreement among answers generated by vector-based, graph-based, and web-based retrieval systems using ROUGE-L and BLEU metrics. Summaries are first generated for each answer. ROUGE-L measures the overlap of key information using the Longest Common Subsequence (LCS), defined as:

|  |  |
| --- | --- |
| (13) |  |

where the numerator represents the length of the LCS between summaries, while the denominator normalizes the score. This metric emphasizes consistency in retaining critical factual information.

BLEU evaluates the localized precision of n-gram matches between summaries, defined as:

|  |  |
| --- | --- |
| (14) |  |

where represents -gram precision, and denotes weight coefficients. This metric excels in detecting precise matches of terminologies or numerical values.

A weighted fusion of and is then applied to balance macro-level semantic alignment with micro-level detail consistency, measuring the similarity between any two answers. If the pairwise similarity exceeds a predefined threshold, the result is refined using a Lightweight Language Model (LLM) to produce the final answer A. The framework proceeds to expert model refinement if the similarity is below the threshold.

Expert Model Refinement. For conflicting answers, the framework employs LLMs, Multimodal LLMs (MLLMs) or Cot-based language models (Cot-LMs) to synthesize a refined response by integrating multi-source evidence. The LLM or MLLM processes the original query and the retrieved evidence to generate the final answer . This step serves as an expert-guidance,ensuring that the final response is both contextually coherent and factually accurate, even when initial answers exhibit discrepancies.

## 4. Experiments

### 4.1. Experimental Setup

Dataset. We conduct experiments across two multimodal reasoning benchmarks spanning divergent modality configurations, including complex question answering (ScienceQA) and crisis event classification (CrisisMMD).

ScienceQA (Lu etal., [2022](https://arxiv.org/html/2504.12330v1#bib.bib41)). This dataset is the first large-scale multimodal benchmark for scientific question answering spanning 3 core disciplines (Natural Science, Social Science, and Formal Science). The dataset contains 21,208 carefully curated examples organized hierarchically across 26 topics, 127 categories, and 379 distinct reasoning skills. Each instance combines textual questions with optional visual contexts (diagrams, charts, or photographs), with a balanced split of 12,726 training, 4,214 validation, and 4,268 test samples. Following the evaluation protocol established in LLaVA (Liu etal., [2023](https://arxiv.org/html/2504.12330v1#bib.bib37)), we report averaged accuracy across all test samples to assess model performance in multimodal understanding and multi-step scientific reasoning. Notably, 34.6% of test questions require simultaneous processing of both visual and textual information to derive correct answers.

CrisisMMD (Alam etal., [2018](https://arxiv.org/html/2504.12330v1#bib.bib3)). This dataset presents a challenging multimodal collection for disaster response applications, comprising approximately 35,000 social media posts containing both visual and textual content from real-world crisis events. It features a comprehensive annotation scheme with seven distinct disaster categories and four granular severity levels. Its unique value lies in capturing authentic user-generated content that preserves natural n

... (truncated)
```
